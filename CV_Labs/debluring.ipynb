{"cells":[{"cell_type":"markdown","metadata":{"id":"QtD8A9Cn0KAx"},"source":["# AIMasters Computer Vision and Video Processing &mdash; Image deblurring task\n","\n","<img src=\"images/blur.png\">\n","\n","In the first task of the course you will have to implement an image deblurring method.  \n","We will start with the simplest possible network and by the end of the task you will implement SOTA deblurring architecture."]},{"cell_type":"markdown","metadata":{"id":"rLBAbTXhyKMO"},"source":["## Preliminaries"]},{"cell_type":"code","source":["!pip install utils"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P0rw9jKt4qao","executionInfo":{"status":"ok","timestamp":1710351389371,"user_tz":-180,"elapsed":8331,"user":{"displayName":"Ninel Yunusova","userId":"08991731127967350653"}},"outputId":"6256b6f6-9c9d-448a-bfa0-1c29a678cd1e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting utils\n","  Downloading utils-1.0.2.tar.gz (13 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: utils\n","  Building wheel for utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for utils: filename=utils-1.0.2-py2.py3-none-any.whl size=13906 sha256=89db1c1c1548861dab18d83953a76e421df6de8a69c091490b64f74cb06688c4\n","  Stored in directory: /root/.cache/pip/wheels/b8/39/f5/9d0ca31dba85773ececf0a7f5469f18810e1c8a8ed9da28ca7\n","Successfully built utils\n","Installing collected packages: utils\n","Successfully installed utils-1.0.2\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":402},"id":"kx_WMOau0KAz","executionInfo":{"status":"error","timestamp":1710351395082,"user_tz":-180,"elapsed":273,"user":{"displayName":"Ninel Yunusova","userId":"08991731127967350653"}},"outputId":"0aa6e0a3-e252-47a9-e317-6d335be82cb5"},"outputs":[{"output_type":"error","ename":"ImportError","evalue":"cannot import name 'download_from_yadisk' from 'utils' (/usr/local/lib/python3.10/dist-packages/utils/__init__.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-402aebbaa418>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload_from_yadisk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mTARGET_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mFILENAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"GoPro.zip\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'download_from_yadisk' from 'utils' (/usr/local/lib/python3.10/dist-packages/utils/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["from utils import download_from_yadisk, test_model\n","import os\n","\n","TARGET_DIR = '.'\n","FILENAME = \"GoPro.zip\"\n","\n","if not os.path.exists(os.path.join(TARGET_DIR, FILENAME)):\n","    # we are going to download 5.2 gb file, downloading will take some time\n","    download_from_yadisk(\n","        short_url='https://disk.yandex.ru/d/v7O5TOsstnsscw',\n","        filename=FILENAME,\n","        target_dir=TARGET_DIR\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"q2mHYKcZ0KA0"},"outputs":[],"source":["!unzip -qq GoPro.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZzBsocBd0KA1"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","import numpy as np\n","plt.rcParams.update({'axes.titlesize': 'small'})\n","\n","import torch, torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","import torchvision\n","from torchvision.utils import make_grid\n","\n","from torchvision import transforms\n","from PIL import Image\n","import os\n","\n","print(f\"GPU: {torch.cuda.is_available()}\")\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"markdown","metadata":{"id":"bN3qpWRK0KA1"},"source":["### Prepare Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p__A8rOf0KA1"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","import numpy as np\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from pathlib import Path\n","\n","class GoProDataset(Dataset):\n","    def __init__(self, root_dir, transform=None):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        blur_path = Path(f\"{root_dir}/blur\")\n","        sharp_path = Path(f\"{root_dir}/sharp\")\n","        self.blurry_images = [p.name for p in list(blur_path.glob(\"*.png\"))]\n","\n","    def __len__(self):\n","        return len(self.blurry_images)\n","\n","    def __getitem__(self, idx):\n","        blurry_path = Path(self.root_dir, 'blur', self.blurry_images[idx])\n","        sharp_path = Path(self.root_dir, 'sharp', self.blurry_images[idx])\n","\n","        blurry_image = np.array(Image.open(blurry_path).convert('RGB'))\n","        sharp_image = np.array(Image.open(sharp_path).convert('RGB'))\n","\n","        if self.transform:\n","            blurry_image = blurry_image.astype(np.float32)/255.0\n","            sharp_image = sharp_image.astype(np.float32)/255.0\n","            augmented = self.transform(image=blurry_image, image1=sharp_image)\n","            blurry_image = augmented['image']\n","            blurry_image = blurry_image.to(torch.float32)\n","            sharp_image = augmented['image1']\n","            sharp_image = sharp_image.to(torch.float32)\n","\n","        return blurry_image, sharp_image\n","\n","transform = A.Compose([\n","    A.RandomCrop(256, 256, p=1.0),\n","    A.HorizontalFlip(p=0.5),\n","    A.VerticalFlip(p=0.5),\n","    ToTensorV2(),\n","], additional_targets={\n","        'image1': 'image'}, p=1)\n","\n","transform_test = A.Compose([\n","    ToTensorV2(),\n","], additional_targets={\n","        'image1': 'image'}, p=1)\n","\n","train_dataset = GoProDataset(root_dir='GoPro/train', transform=transform)\n","train_dataloader = DataLoader(train_dataset, batch_size=3, shuffle=True, num_workers=16)\n","\n","test_dataset = GoProDataset(root_dir='GoPro/test', transform=transform_test)\n","test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BC0Tb_wq0KA2"},"outputs":[],"source":["samples = torch.stack([train_dataset[i][0] for i in range(32, 48)], dim=0)\n","\n","plt.figure(figsize=(10, 10))\n","plt.imshow(make_grid(samples, nrow=4).permute(1, 2, 0))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"cFUhdcvayKMS"},"source":["## Baseline\n","\n","### Basic network (1 point)\n","We will start off by implementing the simplest possible convolutional neural network.  \n","A few convolutional layers, without pooling.\n","\n","Remenicent of early works in [Super-Resolution](https://arxiv.org/abs/1501.00092) and [Denoising](https://arxiv.org/abs/1608.03981)\n","\n","**Important to note that our network will learn the residual for debluring**  \n","$I_{deblur}=I_{input} + f_{\\theta}(I_{input})$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ePYBN_qgyKMS"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class ConvBnBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        \"\"\"Simple convolutional block\n","\n","        Your task is to fill in the following modules:\n","\n","            conv + bn + relu\n","\n","        \"\"\"\n","        super().__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3)\n","        self.bn = nn.BatchNorm2d(out_channels)\n","\n","    def forward(self, x):\n","        # Your code vvv\n","        x = self.conv(x)\n","        x = self.bn(x)\n","        x = F.relu(x)\n","        # Your code ^^^\n","\n","\n","class Baseline(nn.Module):\n","    def __init__(self, block, n_blocks=5, n_filters=64):\n","        \"\"\"Basic convolutional model\n","\n","        Your task is to implement the following architecture:\n","\n","            input_conv(3, n_filters)\n","            n_blocks * ConvBnBlock\n","            output_conv(n_filters, 3)\n","\n","        \"\"\"\n","\n","        super().__init__()\n","        # Your code vvv\n","        # Your code ^^^\n","\n","    def forward(self, x):\n","        inp = x\n","        # Your code vvv\n","        # Your code ^^^\n","        return x + inp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zMGBR1wQyKMT"},"outputs":[],"source":["baseline = Baseline(ConvBnBlock)\n","test_input = torch.rand(1, 3, 256, 256)\n","test_output = baseline(test_input)\n","assert test_input.shape == test_output.shape"]},{"cell_type":"markdown","metadata":{"id":"RTn6OcGcyKMT"},"source":["### PSNR Loss (0.5 points)\n","\n","We will directly optimize the PSNR quality metric instead of plain MSE  \n","$PSNR = 10 * \\log_{10}(\\frac{MAX_I^2}{MSE})$  \n","In out case the images are normed to [0, 1], so the $MAX_I=1$\n","\n","Be careful when averaging, first calculate the MSE for each pair of images, only then apply log and after that average along batch axis."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QYze7pX9yKMT"},"outputs":[],"source":["class PSNRLoss(nn.Module):\n","    def __init__(self):\n","        \"\"\"Peak signal-to-noise ratio loss function\n","\n","        NOTE: during training we minimize the loss but greater PSNR mean better\n","        So you may carry the -1 into the log when implementing\n","        thus elimating the need for division\n","        \"\"\"\n","        super().__init__()\n","        self.eps = 1e-8  # use eps to prevent 0 in log\n","\n","    def forward(self, pred, target):\n","        \"\"\"\n","            Implement the following calculation:\n","\n","                10 * mean(log10(mse(pred, target)))\n","\n","        \"\"\"\n","        # Your code vvv\n","        # Your code ^^^"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BZ6KaiNvyKMT"},"outputs":[],"source":["criterion = PSNRLoss()\n","a = torch.tensor([[[[0.1632, 0.0024, 0.9913, 0.8892],\n","          [0.5655, 0.4472, 0.4592, 0.2013],\n","          [0.7722, 0.9089, 0.1708, 0.3654],\n","          [0.6147, 0.9567, 0.7018, 0.2376]]]])\n","b = torch.tensor([[[[0.8498, 0.1168, 0.3987, 0.6781],\n","          [0.7864, 0.9762, 0.3694, 0.9926],\n","          [0.9000, 0.0293, 0.0454, 0.0984],\n","          [0.9478, 0.3730, 0.9617, 0.5052]]]])\n","assert torch.isclose(criterion(a, b), torch.tensor(-6.8417))"]},{"cell_type":"markdown","metadata":{"id":"YZSQK6IoyKMT"},"source":["### Training (0.5 points)\n","Basic training pipeline.\n","\n","Note the use of gradient clipping, while not required to train the simplest model it will greately help for later parts of the task!\n","\n","Hint: use [`torch.nn.utils.clip_grad_norm_`](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html), `max_norm=0.05` worked fine"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mdgJSKX1yKMT"},"outputs":[],"source":["def train_model(model, train_dataloader, optimizer, criterion, scheduler, num_epochs=200, checkpoints_path=\"./checkpoints\", use_grad_clip=True):\n","    # Create checkpoints folder\n","    PATH = checkpoints_path\n","    os.makedirs(PATH, exist_ok = True)\n","\n","    # Training loop\n","    for epoch in range(num_epochs):\n","        for inputs, targets in train_dataloader:\n","            # Your code vvv\n","\n","            if use_grad_clip:\n","                pass\n","\n","            # Your code ^^^\n","\n","        torch.save({\n","                  'epoch': epoch,\n","                  'model_state_dict': model.state_dict(),\n","                  'optimizer_state_dict': optimizer.state_dict()\n","                  }, os.path.join(PATH, f\"epoch_{epoch+1}.tar\"))\n","\n","        # Update the learning rate\n","        scheduler.step()\n","        print(f'Epoch {epoch+1}, Loss: {loss.item()}, LR: {scheduler.get_last_lr()[0]}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yea6xut_yKMU"},"outputs":[],"source":["from utils import get_scheduler\n","\n","torch.manual_seed(11)\n","\n","model = Baseline(ConvBnBlock)\n","model = model.to(device)\n","\n","criterion = PSNRLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","use_grad_clip = True\n","scheduler = get_scheduler(optimizer)\n","\n","train_model(model, train_dataloader, optimizer, criterion, scheduler, num_epochs=50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CF4P-KBcyKMU"},"outputs":[],"source":["result = test_model(model, device, test_dataloader)\n","assert result <= 0.00252\n","print(\"Congrats!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XzIObnt_yKMU"},"outputs":[],"source":["pics = test_dataset[2]\n","blurred, gt = pics[0], pics[1]\n","plt.figure(figsize=(20, 4))\n","plt.suptitle(\"Visual inspection\")\n","plt.subplot(131)\n","plt.title(\"Blurred\")\n","plt.imshow(blurred.permute(1, 2, 0))\n","plt.subplot(132)\n","plt.title(\"Model result\")\n","with torch.no_grad():\n","    output = model(blurred.unsqueeze(0).to(device)).cpu().squeeze(0).permute(1, 2, 0).numpy()\n","output = np.clip(output, 0, 1)\n","plt.imshow(output)\n","plt.subplot(133)\n","plt.imshow(gt.permute(1, 2, 0))\n","plt.title(\"GT\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"iw76gcvB0KA2"},"source":["## UNet\n","\n","The next step is to process our image in multiple scales, like the multi-scale methods of old school CV.\n","\n","The most common model that uses multiple scales is the [UNet](https://arxiv.org/abs/1505.04597), first proposed for medical segmentation and containing many strange design decisions.  \n","Nonetheless, it has proven to be useful for many CV tasks and is videly used in image and video restoration.\n","\n","U-Nets are named as such because they have this U-like shape, where the input image is first reduced in dimensionality in the downsizing portion, then increased in dimensionality back to its original size in the upsizing portion."]},{"cell_type":"markdown","metadata":{"id":"LVs9rARhyKMU"},"source":["<img src=\"images/unet.png\">\n","\n","\n","UNet may be implemented with either concatenation of skip-connections or with the summation.  \n","The results usually do not differ that much."]},{"cell_type":"markdown","metadata":{"id":"BIh9i6Fe0KA2"},"source":["### Double convolution (0.5 points)\n","In UNet each convolution block is created with two convolutions.  \n","You have to implement a simple stack of two layers of convolution with two ReLU activations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DyBa95_S0KA3"},"outputs":[],"source":["class UNetBlock(nn.Module):\n","    def __init__(self, in_channels):\n","        \"\"\"Basic building block of UNet architecture\n","\n","        Your task is to fill in the following modules:\n","\n","            conv + bn + relu + conv + bn + relu\n","\n","        \"\"\"\n","        super().__init__()\n","        # Your code vvv\n","        # Your code ^^^\n","\n","    def forward(self, x):\n","        # Your code vvv\n","        # Your code ^^^"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-3D16PQxyKMU"},"outputs":[],"source":["block = UNetBlock(64)\n","vec = torch.rand(16, 64, 256, 256)\n","assert block(vec).shape == torch.Size([16, 64, 256, 256])"]},{"cell_type":"markdown","metadata":{"id":"Gow-gnb00KA3"},"source":["### UNet Down block (0.5 points)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gbJgUqsp0KA4"},"outputs":[],"source":["class UNetDownBlock(nn.Module):\n","    def __init__(self, chan):\n","        \"\"\"Downsampling block in the encoder of UNet\n","\n","            Your task is to fill in the following modules:\n","\n","                AvgPool + Conv 1x1\n","\n","            Input spatial dimension is **reduced** by a factor of 2\n","            The number of channels is **increased** by a factor of 2\n","        \"\"\"\n","        super().__init__()\n","        # Your code vvv\n","        # Your code ^^^\n","\n","    def forward(self, x):\n","        # Your code vvv\n","        # Your code ^^^"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E_-hhfnfyKMU"},"outputs":[],"source":["block = UNetDownBlock(64)\n","vec = torch.rand(16, 64, 256, 256)\n","assert block(vec).shape == torch.Size([16, 128, 128, 128])"]},{"cell_type":"markdown","metadata":{"id":"v--zx3tG0KA4"},"source":["### UNet Up block (0.5 points)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j8kD2crg0KA5"},"outputs":[],"source":["class UNetUpBlock(nn.Module):\n","    def __init__(self, chan):\n","        \"\"\"Upsampling block in the encoder of UNet\n","\n","            Your task is to fill in the following modules:\n","\n","                Upsample + Conv2d\n","\n","            Input spatial dimension is **increased** by a factor of 2\n","            The number of channels is **reduced** by a factor of 2\n","        \"\"\"\n","        super(UNetUpBlock, self).__init__()\n","        # Your code vvv\n","        # Your code ^^^\n","\n","    def forward(self, x):\n","        # Your code vvv\n","        # Your code ^^^"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5E4q-e2WyKMV"},"outputs":[],"source":["block = UNetUpBlock(64)\n","vec = torch.rand(16, 64, 256, 256)\n","assert block(vec).shape == torch.Size([16, 32, 512, 512])"]},{"cell_type":"markdown","metadata":{"id":"iU91FD3TyKMV"},"source":["### Generalized Unet arch (1.5 points)\n","Over the years the UNet architecture has proven itself useful.  \n","Many follow up papers retain macro level architecture and change individual blocks, i.e. only changing the downscaling operation, changing number of convs in block, adding attention to bottleneck, etc.  \n","\n","Therefore we will also implement a generalized architecture.\n","\n","**Your model must pass the quality assert to get points for this part**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EOaUxBKCyKMV"},"outputs":[],"source":["class GeneralizedUNet(nn.Module):\n","    def __init__(self, block, downBlock, upBlock, img_channel=3, width=16,\n","                 middle_blk_num=1, enc_blk_nums=[1,1,1,1], dec_blk_nums=[1,1,1,1]):\n","        \"\"\"GeneralizedUNet architecture\n","\n","            This part is implemented for you\n","            But feel free to change it\n","\n","        \"\"\"\n","        super().__init__()\n","\n","        self.intro = nn.Conv2d(in_channels=img_channel, out_channels=width, kernel_size=3, padding=1, stride=1, groups=1, bias=True)\n","        self.ending = nn.Conv2d(in_channels=width, out_channels=img_channel, kernel_size=3, padding=1, stride=1, groups=1, bias=True)\n","\n","        self.encoders = nn.ModuleList()\n","        self.decoders = nn.ModuleList()\n","        self.middle_blks = nn.ModuleList()\n","        self.ups = nn.ModuleList()\n","        self.downs = nn.ModuleList()\n","\n","        chan = width\n","\n","        for num in enc_blk_nums:\n","            self.encoders.append(\n","                nn.Sequential(\n","                    *[block(chan) for _ in range(num)]\n","                )\n","            )\n","            self.downs.append(\n","                downBlock(chan)\n","            )\n","            chan = chan * 2\n","\n","        self.middle_blks = \\\n","            nn.Sequential(\n","                *[block(chan) for _ in range(middle_blk_num)]\n","            )\n","\n","        for num in dec_blk_nums:\n","            self.ups.append(\n","                upBlock(chan)\n","            )\n","            chan = chan // 2\n","            self.decoders.append(\n","                nn.Sequential(\n","                    *[block(chan) for _ in range(num)]\n","                )\n","            )\n","\n","        self.padder_size = 2 ** len(self.encoders)\n","\n","    def forward(self, inp):\n","        \"\"\"Performs forward pass of a UNet-like model\n","\n","        Your task is to implement the following steps:\n","\n","            1. Intro convolution\n","            2. Encoder blocks + save skip connections + downsample\n","            3. Middle blocks (also known as bottleneck)\n","            4. Decoder blocks + add skip connection\n","            5. Ending\n","            6. Inp + ending out (we're still using residual lerning)\n","\n","        \"\"\"\n","        activations = []\n","        # Your code vvv\n","        # Your code ^^^"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4xGnWMn7yKMV"},"outputs":[],"source":["model = GeneralizedUNet(UNetBlock, UNetDownBlock, UNetUpBlock)\n","vec = torch.rand(16, 3, 256, 256)\n","assert model(vec).shape == torch.Size([16, 3, 256, 256])"]},{"cell_type":"markdown","metadata":{"id":"_Uy5g2CWyKMV"},"source":["### Train UNet"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"r-EFnsFYyKMV"},"outputs":[],"source":["torch.manual_seed(11)\n","\n","model = GeneralizedUNet(UNetBlock, UNetDownBlock, UNetUpBlock)\n","model = model.to(device)\n","\n","criterion = PSNRLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","use_grad_clip = True\n","scheduler = get_scheduler(optimizer)\n","\n","train_model(model, train_dataloader, optimizer, criterion, scheduler, num_epochs=100, use_grad_clip=use_grad_clip)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pgRqagoRyKMV"},"outputs":[],"source":["result = test_model(model, device, test_dataloader)\n","assert result <= 0.00235"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ohTlMs8XyKMV"},"outputs":[],"source":["pics = test_dataset[2]\n","blurred, gt = pics[0], pics[1]\n","plt.figure(figsize=(20, 4))\n","plt.suptitle(\"Visual inspection\")\n","plt.subplot(131)\n","plt.title(\"Blurred\")\n","plt.imshow(blurred.permute(1, 2, 0))\n","plt.subplot(132)\n","plt.title(\"Model result\")\n","with torch.no_grad():\n","    output = model(blurred.unsqueeze(0).to(device)).cpu().squeeze(0).permute(1, 2, 0).numpy()\n","output = np.clip(output, 0, 1)\n","plt.imshow(output)\n","plt.subplot(133)\n","plt.imshow(gt.permute(1, 2, 0))\n","plt.title(\"GT\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"x0Gw_IpZ0KA6"},"source":["## NAFNet\n","As discussed in the previous section, many networks build upon UNet and change individual blocks.  \n","[NAFNet](https://arxiv.org/abs/2204.04676) was for a long time a SOTA deblurring approach, even thought it was presented as a baseline.\n","\n","The main differences are:\n","* No regular nonlinearities (no ReLU/GELU/ELU/etc.)\n","* Simplified channel attention\n","* LayerNorm instead of BatchNorm\n","* and many more training tricks taken from transformer papers\n","\n","In a way this work is similar to [ConvNeXt](https://arxiv.org/abs/2201.03545), where authors also shook off the dust of ResNet and trained a SOTA classification CNN.\n","\n","\n","**And now you are tasked with implementing NAFNet :)**"]},{"cell_type":"markdown","metadata":{"id":"Kk4zMxX90KA7"},"source":["### Simple gate block (0.5 points)\n","\n","<img src=\"images/gate.png\">\n","\n","Instead of using regular non-linearities NAFNet proposes the use of \"Simple Gates\" which perform pointwise multiplication of feature maps.\n","\n","Simple gate layer splits input feature map in 2 part along the channel axis and multiplies them.\n","\n","You may find [`torch.chunk`](https://pytorch.org/docs/stable/generated/torch.chunk.html) to be useful"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lcu-ciuO0KA7"},"outputs":[],"source":["class SimpleGate(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        \"\"\"\n","\n","            Split the input in 2, along the channel axis\n","            Return multiplication of these 2 parts\n","\n","        \"\"\"\n","        # Your code vvv\n","        # Your code ^^^"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hRTnQatdyKMa"},"outputs":[],"source":["block = SimpleGate()\n","vec = torch.rand(16, 64, 256, 256)\n","assert block(vec).shape == torch.Size([16, 32, 256, 256])"]},{"cell_type":"markdown","metadata":{"id":"X8gq6YX50KA7"},"source":["### NAFNet up block (0.5 points)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3fqI5WG-0KA7"},"outputs":[],"source":["class NAFNetUpBlock(nn.Module):\n","    def __init__(self, channels):\n","        \"\"\"NAFNet upsampling block\n","\n","        Implement and use the following modules:\n","\n","            conv 1x1 (chan, 2 * chan)\n","            pixelshuffle(2)\n","\n","        \"\"\"\n","        super().__init__()\n","        # Your code vvv\n","        # Your code ^^^\n","    def forward(self, x):\n","        # Your code vvv\n","        # Your code ^^^"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PggeeuCNyKMa"},"outputs":[],"source":["block = NAFNetUpBlock(64)\n","vec = torch.rand(16, 64, 256, 256)\n","assert block(vec).shape == torch.Size([16, 32, 512, 512])"]},{"cell_type":"markdown","metadata":{"id":"PgDdUj810KA8"},"source":["### NAFNet down block (0.5 points)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z9fNVMCc0KA8"},"outputs":[],"source":["class NAFNetDownBlock(nn.Module):\n","    def __init__(self, channels):\n","        \"\"\"NAFNet downsampling block\n","\n","        Implement and use the following modules:\n","\n","            conv with stride 2, **mind the padding**\n","\n","        \"\"\"\n","        super().__init__()\n","        # Your code vvv\n","        # Your code ^^^\n","\n","    def forward(self, x):\n","        # Your code vvv\n","        # Your code ^^^"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZvKPbYFeyKMb"},"outputs":[],"source":["block = NAFNetDownBlock(64)\n","vec = torch.rand(16, 64, 256, 256)\n","assert block(vec).shape == torch.Size([16, 128, 128, 128])"]},{"cell_type":"markdown","metadata":{"id":"HNgwNdvA0KA9"},"source":["### Simplified channel attention (0.5 points)\n","\n","Regular channel attention produces weights for each channel of input feature map using 2-layer MLP.  \n","NAFNet authors propose to remove the small MLP on channels by just pooling and linearly projecting the feature map to get channel weights.\n","\n","<img src=\"images/sca.png\">"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DmvdnirC0KA9"},"outputs":[],"source":["class SCA(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        \"\"\"Simplified channel attention module\n","\n","        Implement and use the following modules:\n","\n","            adaptiveavgpool to get 1x1 feature map\n","            conv 1x1 projection layer\n","\n","        \"\"\"\n","        super().__init__()\n","        # Your code vvv\n","        # Your code ^^^\n","    def forward(self, x):\n","        \"\"\"\n","\n","            Return only the attention weights\n","\n","        \"\"\"\n","        # Your code vvv\n","        # Your code ^^^"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a-hXRF64yKMb"},"outputs":[],"source":["block = SCA(64, 32)\n","vec = torch.rand(16, 64, 256, 256)\n","assert block(vec).shape == torch.Size([16, 32, 256, 256])"]},{"cell_type":"markdown","metadata":{"id":"Oo1Nv3810KA9"},"source":["### NAFNet block (3 points)\n","\n","**The final boss of the task**.\n","\n","The diagram shows the intra-block structure:\n","\n","<img src=\"images/nafnet_block.png\" width=620>\n","\n","Note the use of learnable skip-connection scales `beta` and `gamma`, use [`nn.Parameter`](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html)\n","\n","**Your model must pass the quality assert to get points for this part**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AFbYaXuy0KA-"},"outputs":[],"source":["from utils import LayerNorm2d  # use this layernorm\n","\n","class NAFNetBlock(nn.Module):\n","    def __init__(self, c, DW_Expand=2, FFN_Expand=2):\n","        super().__init__()\n","\n","        dw_channel = c * DW_Expand\n","        ffn_channel = FFN_Expand * c\n","\n","        self.beta = nn.Parameter(...)\n","        self.gamma = nn.Parameter(...)\n","\n","        # Your code vvv\n","        # Your code ^^^\n","\n","    def forward(self, inp):\n","        # Your code vvv\n","        # Your code ^^^"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7lv5MF2LyKMb"},"outputs":[],"source":["block = NAFNetBlock(64)\n","vec = torch.rand(16, 64, 256, 256)\n","assert block(vec).shape == torch.Size([16, 64, 256, 256])"]},{"cell_type":"markdown","metadata":{"id":"SAHl5Mm0yKMb"},"source":["### Train NAFNet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7bXcC7cW0KA-"},"outputs":[],"source":["torch.manual_seed(11)\n","\n","model = GeneralizedUNet(NAFNetBlock, NAFNetDownBlock, NAFNetUpBlock, enc_blk_nums=[1,2,2,28], dec_blk_nums=[2,2,2,1])\n","model = model.to(device)\n","\n","criterion = PSNRLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","use_grad_clip = True\n","scheduler = get_scheduler(optimizer)\n","\n","train_model(model, train_dataloader, optimizer, criterion, scheduler)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"30zKG0KIyKMb"},"outputs":[],"source":["from utils import test_model\n","result = test_model(model, device, test_dataloader)\n","assert result <= 0.002"]},{"cell_type":"markdown","metadata":{"id":"bHt8PNJ1yKMc"},"source":["## Check model results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rPb4_RmpyKMc"},"outputs":[],"source":["pics = test_dataset[2]\n","blurred, gt = pics[0], pics[1]\n","plt.figure(figsize=(20, 4))\n","plt.suptitle(\"Visual inspection\")\n","plt.subplot(131)\n","plt.title(\"Blurred\")\n","plt.imshow(blurred.permute(1, 2, 0))\n","plt.subplot(132)\n","plt.title(\"Model result\")\n","with torch.no_grad():\n","    output = model(blurred.unsqueeze(0).to(device)).cpu().squeeze(0).permute(1, 2, 0).numpy()\n","output = np.clip(output, 0, 1)\n","plt.imshow(output)\n","plt.subplot(133)\n","plt.imshow(gt.permute(1, 2, 0))\n","plt.title(\"GT\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"WOZFFM16yKMc"},"source":["# Your thoughts\n","\n","Please share your thoughts on the task.  \n","Did you like it, did you hate it?\n","\n","If so, why?"]},{"cell_type":"markdown","metadata":{"id":"E16kxEcJyKMc"},"source":["**your text goes here**"]}],"metadata":{"accelerator":"TPU","anaconda-cloud":{},"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"notebookId":"d939206a-bf6c-4507-b691-b1019e21de8e"},"nbformat":4,"nbformat_minor":0}