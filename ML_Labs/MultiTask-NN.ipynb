{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f79147-0c66-4c5d-ade3-a88575b824d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ydata_profiling==4.5.1\n",
    "!pip install matplotlib==3.7.3\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install sklearn\n",
    "!pip install category_encoders\n",
    "!pip install imblearn\n",
    "!pip install tabulate\n",
    "!pip install seaborn\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d75243d9-cc06-4bd8-bb5a-bb9ade971ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import warnings\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, Ridge\n",
    "from ydata_profiling import ProfileReport\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim # for optimizer\n",
    "from torch.utils.tensorboard import SummaryWriter #for Tensorboard\n",
    "from torch.nn import MSELoss, L1Loss\n",
    "from torch.optim import SGD, Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Отключить все предупреждения\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177abca7-0f89-4da6-ad83-e2536cb07269",
   "metadata": {},
   "source": [
    "# Task 1.1: Multi-task deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6fbbc2-4c3e-4006-b2cf-cd4157f8c641",
   "metadata": {},
   "source": [
    "Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d1947844-07dc-486d-9a93-dc28b82a0ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('lateness_data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb9a296-3121-4cf6-8117-bba1adc2cf36",
   "metadata": {},
   "source": [
    "Make a report. Didn't find some outliers. Not have significant imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0520ad4d-ccbc-49da-a707-8b9bf0b3949f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarize dataset: 100%|█████████████| 86/86 [00:07<00:00, 11.23it/s, Completed]\n",
      "Generate report structure: 100%|██████████████████| 1/1 [00:01<00:00,  1.70s/it]\n",
      "Render HTML: 100%|████████████████████████████████| 1/1 [00:00<00:00,  1.22it/s]\n",
      "Export report to file: 100%|█████████████████████| 1/1 [00:00<00:00, 116.59it/s]\n"
     ]
    }
   ],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "profile = ProfileReport(df, title=\"Profiling Report\")\n",
    "profile.to_file(\"lateness_data.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a918bdd-f6eb-4886-8318-fa7542ffa44a",
   "metadata": {},
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8716dd-0dd6-490a-9a64-1b7dddcd8dac",
   "metadata": {},
   "source": [
    "Encode categorical columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0a607e94-36c5-4623-a118-265ace053679",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['direct_delivery'] = label_encoder.fit_transform(df['direct_delivery'])\n",
    "df['batched_pickup'] = label_encoder.fit_transform(df['batched_pickup'])\n",
    "df['status'] = label_encoder.fit_transform(df['status'])\n",
    "\n",
    "custom_mapping = {\n",
    "    'automobile': 4,\n",
    "    'bicycle': 3,\n",
    "    'scooter': 2,\n",
    "    'foot': 1,\n",
    "}\n",
    "df['transport_type'] = df['transport_type'].map(custom_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47b80d1-7ff1-4c78-81f2-e2da5a543467",
   "metadata": {},
   "source": [
    "Make new feature instead two datetimes features. Retrieve day, month and hour of order status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9159afe9-ce9f-47c7-9374-36f3d770350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['order_time'] = pd.to_datetime(df['order_time'])\n",
    "\n",
    "df['order_day'] = df['order_time'].dt.day\n",
    "df['order_month'] = df['order_time'].dt.month\n",
    "df['order_hour'] = df['order_time'].dt.hour\n",
    "\n",
    "#sort by time like time series and delete this feature (not meaningfull)\n",
    "df.sort_values(by=['order_time'], inplace=True)\n",
    "df.drop(['order_time'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed5a342-723d-43f8-816c-8e3d6238f17b",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "62a135f2-c553-4ac6-9d1c-b1ce37b5e12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5050b205-8564-4587-9012-b9ffab58b416",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7975e105-1738-439f-867e-6235ffc14ea0",
   "metadata": {},
   "source": [
    "It scales and transforms features (variables) in a dataset so that they fall within a specific range, StandardScaler: between -1 and 1. Deep learning models often include batch normalization layers, which can adapt to different scales during training. However, it's still a good practice to standardize your input data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7c8a93ee-fac1-4dfd-8536-7aac3f50d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_scale = ['delivery_distance', 'order_preparation_time', 'responsible_id', 'store_latitude', 'store_longitude', 'client_latitude', 'client_longitude', 'order_day', 'order_month', 'order_hour']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train[columns_to_scale])\n",
    "train[columns_to_scale] = scaler.transform(train[columns_to_scale])\n",
    "test[columns_to_scale] = scaler.transform(test[columns_to_scale])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da15ea22-7b35-45bc-85fd-ab0a7d7e34e0",
   "metadata": {},
   "source": [
    "### Feature Selecting\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) is a regularization technique used in linear regression and other machine learning models to select important features and prevent overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "95e308e2-9b86-4eab-8fb2-ee385d3ef749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('direct_delivery', 1.7046368152150357), ('order_preparation_time', 0.7658858162604002), ('batched_pickup', 0.6962759992259239), ('order_month', 0.46963854823090223), ('order_day', 0.18190807784478655), ('store_longitude', 0.10569262556105394), ('order_hour', -0.09607567851672215), ('responsible_id', 0.072734150790699), ('delivery_distance', 0.07162370604466285), ('transport_type', -0.0), ('store_latitude', 0.0), ('client_latitude', 0.0), ('client_longitude', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso(alpha=0.01)\n",
    "# Fit the Lasso model on the training data for REGRESSION task\n",
    "X_train = train.drop(['status_time', 'status'], axis = 1)\n",
    "lasso.fit(X_train, train['status_time'])\n",
    "# Get the feature importances or coefficients\n",
    "feature_importances = lasso.coef_\n",
    "importance_dict = dict(zip(X_train.columns, feature_importances))\n",
    "sorted_importance = sorted(importance_dict.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "print(sorted_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e9f2a04f-8e33-4aad-877e-327246ebcd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('direct_delivery', -0.24282146206339947), ('order_preparation_time', -0.08341745349942899), ('batched_pickup', -0.023131345996951994), ('delivery_distance', 0.01871273858638263), ('order_month', -0.01847353742267497), ('client_latitude', 0.017151125508079717), ('responsible_id', -0.010556054423950844), ('store_longitude', -0.003971829480059397), ('order_hour', 0.00015904912685032446), ('transport_type', 0.0), ('store_latitude', 0.0), ('client_longitude', -0.0), ('order_day', -0.0)]\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso(alpha=0.01)\n",
    "# Fit the Lasso model on the training data for CLASSIFICATION task\n",
    "lasso.fit(X_train, train['status'])\n",
    "# Get the feature importances or coefficients\n",
    "feature_importances = lasso.coef_\n",
    "importance_dict = dict(zip(X_train.columns, feature_importances))\n",
    "sorted_importance = sorted(importance_dict.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "print(sorted_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd9635f-43b5-411d-ab0f-479cebddd4f4",
   "metadata": {},
   "source": [
    "For both task we don't need columns (has zeroes coeficient): store_latitude, client_longitude, responsible_id, transport_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "be441d04-47d3-437d-910a-96e6df78f6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['responsible_id', 'transport_type', 'store_latitude', 'client_latitude'], axis = 1, inplace=True) \n",
    "test.drop(['responsible_id', 'transport_type', 'store_latitude', 'client_latitude'], axis = 1, inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d2dfa5-718d-4a39-b440-ac0a0df1d7e1",
   "metadata": {},
   "source": [
    "### Make Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0df9c0fc-68aa-4491-a61e-ca854039f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataframe.iloc[idx]\n",
    "        inputs = torch.Tensor(sample[features_columns])  # Extract input features from DataFrame\n",
    "        classification_label = torch.tensor([sample[classification_column]], dtype=torch.int64)  # Extract classification label from DataFrame\n",
    "        regression_label = torch.Tensor([sample[regression_column]])  # Extract regression label from DataFrame\n",
    "\n",
    "        return inputs, classification_label, regression_label\n",
    "\n",
    "\n",
    "# Names of DataFrame columns\n",
    "features_columns = [name for name in train.columns if name not in ['status','status_time']]\n",
    "classification_column = 'status' \n",
    "regression_column = 'status_time'  \n",
    "\n",
    "# Create a CustomDataset instance\n",
    "custom_dataset_train = CustomDataset(train)\n",
    "custom_dataset_test = CustomDataset(test)\n",
    "\n",
    "# Create a dataloader\n",
    "batch_size = 64  # Specify the batch size - CHANGE\n",
    "train_dataloader = DataLoader(custom_dataset_train, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(custom_dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0612bb-e049-4415-837d-5f39b66521ca",
   "metadata": {},
   "source": [
    "### Multi-Task Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb0790d-d8cf-4731-a33d-0eb395c4b6b2",
   "metadata": {},
   "source": [
    "Input we had 9 columns. Model has 5 layers. In output layer we have 4 neuros, 3 for classification task (probability for each class) and 1 for regression task. Loss will be the sum of losses for regression and classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3136d338-1b16-4bdf-af45-bff8ea4781ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMultiTaskNet1(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MyMultiTaskNet1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.fc5 = nn.Linear(32, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        classification_output, regression_output = x.split([3, 1], dim=1)\n",
    "        classification_output = torch.nn.functional.log_softmax(classification_output, dim=1)\n",
    "        return classification_output, regression_output\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model_nn = MyMultiTaskNet1(input_size = len(train.columns) - 2).to(device)\n",
    "number_of_model = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5caecc9c-cd9b-44ef-b46a-f1642395bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, device, train_loader, optimizer, epoch, log_interval=700, alpha=0.2, beta=0.8):\n",
    "    train_loss_class = []\n",
    "    train_loss_reg = []\n",
    "    train_loss_total = []\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    mae_reg_error = []\n",
    "    model.train()\n",
    "    for batch_idx, (data, class_target, reg_target) in enumerate(train_loader):\n",
    "        data, class_target, reg_target = data.to(device), class_target.to(device), reg_target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        class_output, reg_output = model(data)\n",
    "\n",
    "        class_target = class_target.reshape(1, -1).squeeze()\n",
    "        class_loss = F.nll_loss(class_output, class_target)\n",
    "        reg_loss = criterion_regression(reg_output, reg_target)\n",
    "\n",
    "        loss = alpha * class_loss + beta * reg_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        mae_reg_error.append(mean_absolute_error(reg_target, reg_output.detach().numpy()))\n",
    "        \n",
    "        train_loss_class.append(class_loss.item())\n",
    "        train_loss_reg.append(reg_loss.item())\n",
    "        train_loss_total.append(loss.item())\n",
    "        \n",
    "        _, predictions = torch.max(class_output, 1)\n",
    "        all_labels.extend(class_target.cpu().numpy())\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader)}%)]\\tLoss: {loss.item()}, Class Loss: {class_loss.item()}, Regression Loss: {reg_loss.item()}\")\n",
    "\n",
    "    MAE = sum(mae_reg_error) / len(mae_reg_error)\n",
    "    train_loss_class = sum(train_loss_class) / len(train_loss_class)\n",
    "    train_loss_reg = sum(train_loss_reg) / len(train_loss_reg)\n",
    "    train_loss_total = sum(train_loss_total) / len(train_loss_total)\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "    print(f\"TRAIN: MAE = {MAE}   Accuracy = {accuracy}    F1 = {f1}\\n\\n\")\n",
    "\n",
    "    writer.add_scalar(f'Training Class Loss (MyMultiTaskNet{number_of_model})', train_loss_class, epoch)\n",
    "    writer.add_scalar(f'Training Reg Loss (MyMultiTaskNet{number_of_model})', train_loss_reg, epoch)\n",
    "    writer.add_scalar(f'Training Total Loss (MyMultiTaskNet{number_of_model})', train_loss_total, epoch)\n",
    "    writer.add_scalar(f'Training MAE (MyMultiTaskNet{number_of_model})', MAE, epoch)\n",
    "    writer.add_scalar(f'Training Accuracy (MyMultiTaskNet{number_of_model})', accuracy, epoch)\n",
    "    writer.add_scalar(f'Training F1 Score (MyMultiTaskNet{number_of_model})', f1, epoch)\n",
    "    \n",
    "def testModel(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss_class = []\n",
    "    test_loss_reg = []\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    mae_reg_error = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, class_target, reg_target) in enumerate(test_dataloader):\n",
    "            data, class_target, reg_target = data.to(device), class_target.to(device), reg_target.to(device)\n",
    "            class_output, reg_output = model(data)\n",
    "\n",
    "            class_target = class_target.reshape(1, -1).squeeze()\n",
    "            class_loss = torch.nn.functional.nll_loss(class_output, class_target, reduction='sum').item()  # sum up batch loss\n",
    "            reg_loss = criterion_regression(reg_output, reg_target)\n",
    "\n",
    "            test_loss_class.append(class_loss)\n",
    "            test_loss_reg.append(reg_loss.item())\n",
    "        \n",
    "            _, predictions = torch.max(class_output, 1)\n",
    "            all_labels.extend(class_target.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            \n",
    "            mae_reg_error.append(mean_absolute_error(reg_target, reg_output))\n",
    "\n",
    "    MAE = sum(mae_reg_error) / len(mae_reg_error)\n",
    "    test_loss_class = sum(test_loss_class) / len(test_loss_class)\n",
    "    test_loss_reg = sum(test_loss_reg) / len(test_loss_reg)\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "    print(f\"TEST: MAE = {MAE}   Accuracy = {accuracy}    F1 = {f1}\\n\\n\")\n",
    "\n",
    "    writer.add_scalar(f'Testing Class Loss (MyMultiTaskNet{number_of_model})', test_loss_class, epoch)\n",
    "    writer.add_scalar(f'Testing Reg Loss (MyMultiTaskNet{number_of_model})', test_loss_reg, epoch)\n",
    "    writer.add_scalar(f'Testing MAE (MyMultiTaskNet{number_of_model})', MAE, epoch)\n",
    "    writer.add_scalar(f'Testing Accuracy (MyMultiTaskNet{number_of_model})', accuracy, epoch)\n",
    "    writer.add_scalar(f'Testing F1 Score (MyMultiTaskNet{number_of_model})', f1, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a623f48b-e316-4757-a55b-f9e54dbb7c03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/88610 (0.0%)]\tLoss: 48.00542449951172, Class Loss: 1.119350552558899, Regression Loss: 94.89149475097656\n",
      "Train Epoch: 1 [12800/88610 (14.440433212996389%)]\tLoss: 25.21001434326172, Class Loss: 1.0798407793045044, Regression Loss: 49.340187072753906\n",
      "Train Epoch: 1 [25600/88610 (28.880866425992778%)]\tLoss: 30.36240005493164, Class Loss: 1.132720947265625, Regression Loss: 59.592079162597656\n",
      "Train Epoch: 1 [38400/88610 (43.32129963898917%)]\tLoss: 27.59375762939453, Class Loss: 1.1161408424377441, Regression Loss: 54.071372985839844\n",
      "Train Epoch: 1 [51200/88610 (57.761732851985556%)]\tLoss: 20.90045166015625, Class Loss: 1.0789941549301147, Regression Loss: 40.72190856933594\n",
      "Train Epoch: 1 [64000/88610 (72.20216606498195%)]\tLoss: 31.090763092041016, Class Loss: 1.0772238969802856, Regression Loss: 61.10430145263672\n",
      "Train Epoch: 1 [76800/88610 (86.64259927797833%)]\tLoss: 30.701953887939453, Class Loss: 1.091650366783142, Regression Loss: 60.312255859375\n",
      "TRAIN: MAE = 6.164964958907034   Accuracy = 0.3753978106308543    F1 = 0.3698561615191156\n",
      "\n",
      "\n",
      "TEST: MAE = 6.573940608274696   Accuracy = 0.4393084458086941    F1 = 0.3499492502266904\n",
      "\n",
      "\n",
      "Train Epoch: 2 [0/88610 (0.0%)]\tLoss: 30.19313621520996, Class Loss: 1.1538914442062378, Regression Loss: 59.23237991333008\n",
      "Train Epoch: 2 [12800/88610 (14.440433212996389%)]\tLoss: 25.09736442565918, Class Loss: 1.0735292434692383, Regression Loss: 49.12120056152344\n",
      "Train Epoch: 2 [25600/88610 (28.880866425992778%)]\tLoss: 29.697778701782227, Class Loss: 1.0955760478973389, Regression Loss: 58.29998016357422\n",
      "Train Epoch: 2 [38400/88610 (43.32129963898917%)]\tLoss: 27.271181106567383, Class Loss: 1.1233899593353271, Regression Loss: 53.41897201538086\n",
      "Train Epoch: 2 [51200/88610 (57.761732851985556%)]\tLoss: 21.63762092590332, Class Loss: 1.0344675779342651, Regression Loss: 42.24077606201172\n",
      "Train Epoch: 2 [64000/88610 (72.20216606498195%)]\tLoss: 32.68741989135742, Class Loss: 1.0313830375671387, Regression Loss: 64.34346008300781\n",
      "Train Epoch: 2 [76800/88610 (86.64259927797833%)]\tLoss: 30.680889129638672, Class Loss: 1.1108230352401733, Regression Loss: 60.250953674316406\n",
      "TRAIN: MAE = 6.123937360818636   Accuracy = 0.42351879020426586    F1 = 0.41948805661089594\n",
      "\n",
      "\n",
      "TEST: MAE = 6.509818052352333   Accuracy = 0.4726222182097233    F1 = 0.4536517296642787\n",
      "\n",
      "\n",
      "Train Epoch: 3 [0/88610 (0.0%)]\tLoss: 24.236957550048828, Class Loss: 1.0472630262374878, Regression Loss: 47.42665100097656\n",
      "Train Epoch: 3 [12800/88610 (14.440433212996389%)]\tLoss: 25.245452880859375, Class Loss: 1.025012493133545, Regression Loss: 49.46589279174805\n",
      "Train Epoch: 3 [25600/88610 (28.880866425992778%)]\tLoss: 29.564525604248047, Class Loss: 1.0308932065963745, Regression Loss: 58.09815979003906\n",
      "Train Epoch: 3 [38400/88610 (43.32129963898917%)]\tLoss: 27.311241149902344, Class Loss: 1.1158838272094727, Regression Loss: 53.50659942626953\n",
      "Train Epoch: 3 [51200/88610 (57.761732851985556%)]\tLoss: 21.95247459411621, Class Loss: 1.0283534526824951, Regression Loss: 42.87659454345703\n",
      "Train Epoch: 3 [64000/88610 (72.20216606498195%)]\tLoss: 32.119346618652344, Class Loss: 0.99701327085495, Regression Loss: 63.24168014526367\n",
      "Train Epoch: 3 [76800/88610 (86.64259927797833%)]\tLoss: 30.74781036376953, Class Loss: 1.0945578813552856, Regression Loss: 60.40106201171875\n",
      "TRAIN: MAE = 6.1083176855576164   Accuracy = 0.45288342173569573    F1 = 0.44182111198254415\n",
      "\n",
      "\n",
      "TEST: MAE = 6.478793904142353   Accuracy = 0.4765494515415519    F1 = 0.4559696772765077\n",
      "\n",
      "\n",
      "Train Epoch: 4 [0/88610 (0.0%)]\tLoss: 22.75299072265625, Class Loss: 1.0465537309646606, Regression Loss: 44.45942687988281\n",
      "Train Epoch: 4 [12800/88610 (14.440433212996389%)]\tLoss: 25.37375831604004, Class Loss: 1.0253926515579224, Regression Loss: 49.72212219238281\n",
      "Train Epoch: 4 [25600/88610 (28.880866425992778%)]\tLoss: 29.48313331604004, Class Loss: 1.046093225479126, Regression Loss: 57.92017364501953\n",
      "Train Epoch: 4 [38400/88610 (43.32129963898917%)]\tLoss: 27.41678237915039, Class Loss: 1.1075485944747925, Regression Loss: 53.726016998291016\n",
      "Train Epoch: 4 [51200/88610 (57.761732851985556%)]\tLoss: 22.07709503173828, Class Loss: 1.0233949422836304, Regression Loss: 43.130794525146484\n",
      "Train Epoch: 4 [64000/88610 (72.20216606498195%)]\tLoss: 31.653217315673828, Class Loss: 0.9876803159713745, Regression Loss: 62.318756103515625\n",
      "Train Epoch: 4 [76800/88610 (86.64259927797833%)]\tLoss: 30.788623809814453, Class Loss: 1.0856839418411255, Regression Loss: 60.49156188964844\n",
      "TRAIN: MAE = 6.098667332411673   Accuracy = 0.4604107888500169    F1 = 0.4491056611242377\n",
      "\n",
      "\n",
      "TEST: MAE = 6.462976311400576   Accuracy = 0.47334446801787566    F1 = 0.44845008280833704\n",
      "\n",
      "\n",
      "Train Epoch: 5 [0/88610 (0.0%)]\tLoss: 22.420143127441406, Class Loss: 1.0602662563323975, Regression Loss: 43.78002166748047\n",
      "Train Epoch: 5 [12800/88610 (14.440433212996389%)]\tLoss: 25.539653778076172, Class Loss: 1.0310593843460083, Regression Loss: 50.048248291015625\n",
      "Train Epoch: 5 [25600/88610 (28.880866425992778%)]\tLoss: 29.429420471191406, Class Loss: 1.046310305595398, Regression Loss: 57.812530517578125\n",
      "Train Epoch: 5 [38400/88610 (43.32129963898917%)]\tLoss: 27.4896297454834, Class Loss: 1.1040854454040527, Regression Loss: 53.87517547607422\n",
      "Train Epoch: 5 [51200/88610 (57.761732851985556%)]\tLoss: 22.13748550415039, Class Loss: 1.022661566734314, Regression Loss: 43.2523078918457\n",
      "Train Epoch: 5 [64000/88610 (72.20216606498195%)]\tLoss: 31.13266944885254, Class Loss: 0.9852403998374939, Regression Loss: 61.28009796142578\n",
      "Train Epoch: 5 [76800/88610 (86.64259927797833%)]\tLoss: 30.7889461517334, Class Loss: 1.068795084953308, Regression Loss: 60.509098052978516\n",
      "TRAIN: MAE = 6.091481416406184   Accuracy = 0.46179889403001917    F1 = 0.45392903491544195\n",
      "\n",
      "\n",
      "TEST: MAE = 6.452321639322067   Accuracy = 0.4717194059495328    F1 = 0.4364094009555219\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#alpha=0.2 batch_size = 64\n",
    "epochs = 5\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "log_interval = 200\n",
    "criterion_regression = nn.MSELoss()\n",
    "#optimizer = optim.SGD(model_nn.parameters(), lr=lr, momentum=momentum)\n",
    "optimizer = optim.Adam(model_nn.parameters(), lr=0.001)\n",
    "writer = SummaryWriter(\"/Users/ninelco/Documents/Innopolis/F23/ML/Assignment2\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    trainModel(model_nn, device, train_dataloader, optimizer, epoch, log_interval, alpha=0.5, beta=0.5)\n",
    "    testModel(model_nn, device, test_dataloader)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf89db01-4969-4c36-a9f7-a206ac9e0e17",
   "metadata": {},
   "source": [
    "# Task 1.2: Cascade deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e60c4d-597a-47f7-b23a-48c6ce04c530",
   "metadata": {},
   "source": [
    "At first linear model will predict the status of order. Then our X with predict status will be input for linear model for regression model. Both models have own loss and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8790d541-0978-4f47-af81-41cf769d1f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassClassificationModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(len(train.columns) - 2, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        class_output = self.fc4(x)\n",
    "        return torch.nn.functional.log_softmax(class_output, dim=1)\n",
    "\n",
    "class TimePredictionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TimePredictionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(len(train.columns) - 1, 512)\n",
    "        self.drop_1 = nn.Dropout2d(p=0.4)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.drop_2 = nn.Dropout2d(p=0.2)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        self.fc6 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.drop_1(self.fc1(x)))\n",
    "        x = torch.tanh(self.drop_2(self.fc2(x)))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.tanh(self.fc4(x))\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        time_output = self.fc6(x)\n",
    "        return time_output\n",
    "\n",
    "model_classs = ClassClassificationModel()\n",
    "model_reg = TimePredictionModel()\n",
    "number_of_model = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bd0dec6b-30c6-44bc-abb1-b11d90e7bc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainCascade(model_class, model_reg, device, train_loader, optimizer_class, optimizer_reg, epoch, log_interval=700, alpha=0.2, beta=0.8):\n",
    "    model_class.train()\n",
    "    model_reg.train()\n",
    "    train_loss_class = []\n",
    "    train_loss_reg = []\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    mae_reg_error = []\n",
    "    \n",
    "    for batch_idx, (data, class_target, reg_target) in enumerate(train_loader):\n",
    "        data, class_target, reg_target = data.to(device), class_target.to(device), reg_target.to(device)\n",
    "        optimizer_class.zero_grad()\n",
    "        optimizer_reg.zero_grad()\n",
    "        \n",
    "        class_output = model_class(data)\n",
    "        class_target = class_target.reshape(1, -1).squeeze()\n",
    "        class_loss = F.nll_loss(class_output, class_target)\n",
    "\n",
    "        _,pred_t = torch.max(class_output, dim=1)\n",
    "        reg_output = model_reg(torch.cat((data, pred_t.unsqueeze(1)), dim=1))\n",
    "        reg_loss = criterion_regression(reg_output, reg_target)\n",
    "\n",
    "        class_loss.backward()\n",
    "        reg_loss.backward()\n",
    "        \n",
    "        optimizer_class.step()\n",
    "        optimizer_reg.step()\n",
    "\n",
    "        train_loss_class.append(class_loss.item())\n",
    "        train_loss_reg.append(reg_loss.item())\n",
    "        \n",
    "        all_labels.extend(class_target.cpu().numpy())\n",
    "        all_predictions.extend(pred_t.cpu().numpy())\n",
    "\n",
    "        mae_reg_error.append(mean_absolute_error(reg_target, reg_output.detach().numpy()))\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss Class: {:.6f} \\tLoss Reg: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), class_loss.item(), reg_loss.item()))\n",
    "            \n",
    "    MAE = sum(mae_reg_error) / len(mae_reg_error)\n",
    "    train_loss_class = sum(train_loss_class) / len(train_loss_class)\n",
    "    train_loss_reg = sum(train_loss_reg) / len(train_loss_reg)\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "    print(f\"TRAIN: MAE = {MAE}   Accuracy = {accuracy}    F1 = {f1}\\n\\n\")\n",
    "    \n",
    "    writer.add_scalar(f'Training Class Loss (Cascade{number_of_model})', train_loss_class, epoch)\n",
    "    writer.add_scalar(f'Training Reg Loss (Cascade1{number_of_model})', train_loss_reg, epoch)\n",
    "    writer.add_scalar(f'Training MAE (Cascade{number_of_model})', MAE, epoch)\n",
    "    writer.add_scalar(f'Training Accuracy (Cascade{number_of_model})', accuracy, epoch)\n",
    "    writer.add_scalar(f'Training F1 Score (Cascade{number_of_model})', f1, epoch)\n",
    "\n",
    "def testCascade(model_classs, model_reg, device, test_loader):\n",
    "    model_classs.eval()\n",
    "    model_reg.eval()\n",
    "    test_loss_class = []\n",
    "    test_loss_reg = []\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    mae_reg_error = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, class_target, reg_target) in enumerate(test_dataloader):\n",
    "            data, class_target, reg_target = data.to(device), class_target.to(device), reg_target.to(device)\n",
    "            \n",
    "            class_output = model_classs(data)\n",
    "            _,pred_t = torch.max(class_output, dim=1)\n",
    "            reg_output = model_reg(torch.cat((data, pred_t.unsqueeze(1)), dim=1))\n",
    "            \n",
    "            class_target = class_target.reshape(1, -1).squeeze()\n",
    "            class_loss = torch.nn.functional.nll_loss(class_output, class_target, reduction='sum').item()  # sum up batch loss\n",
    "            reg_loss = criterion_regression(reg_output, reg_target)\n",
    "\n",
    "            test_loss_class.append(class_loss)\n",
    "            test_loss_reg.append(reg_loss.item())\n",
    "        \n",
    "            _, predictions = torch.max(class_output, 1)\n",
    "            all_labels.extend(class_target.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            \n",
    "            mae_reg_error.append(mean_absolute_error(reg_target, reg_output))\n",
    "    \n",
    "    MAE = sum(mae_reg_error) / len(mae_reg_error)\n",
    "    test_loss_class = sum(test_loss_class) / len(test_loss_class)\n",
    "    test_loss_reg = sum(test_loss_reg) / len(test_loss_reg)\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "    print(f\"TEST: MAE = {MAE}   Accuracy = {accuracy}    F1 = {f1}\\n\\n\")\n",
    "    writer.add_scalar(f'Testing Class Loss (MyMultiTaskNet{number_of_model})', test_loss_class, epoch)\n",
    "    writer.add_scalar(f'Testing Reg Loss (MyMultiTaskNet{number_of_model})', test_loss_reg, epoch)\n",
    "    writer.add_scalar(f'Testing MAE (MyMultiTaskNet{number_of_model})', MAE, epoch)\n",
    "    writer.add_scalar(f'Testing Accuracy (MyMultiTaskNet{number_of_model})', accuracy, epoch)\n",
    "    writer.add_scalar(f'Testing F1 Score (MyMultiTaskNet{number_of_model})', f1, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7a4248e5-ebe2-48c8-80b6-9626f0fd721f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/88610 (0%)]\tLoss Class: 1.134600 \tLoss Reg: 95.904312\n",
      "Train Epoch: 1 [12800/88610 (14%)]\tLoss Class: 1.026243 \tLoss Reg: 48.270168\n",
      "Train Epoch: 1 [25600/88610 (29%)]\tLoss Class: 1.030714 \tLoss Reg: 58.410873\n",
      "Train Epoch: 1 [38400/88610 (43%)]\tLoss Class: 1.104360 \tLoss Reg: 53.593449\n",
      "Train Epoch: 1 [51200/88610 (58%)]\tLoss Class: 1.027655 \tLoss Reg: 49.402344\n",
      "Train Epoch: 1 [64000/88610 (72%)]\tLoss Class: 1.020731 \tLoss Reg: 65.294365\n",
      "Train Epoch: 1 [76800/88610 (87%)]\tLoss Class: 1.105832 \tLoss Reg: 59.156567\n",
      "TRAIN: MAE = 6.3166157386793556   Accuracy = 0.4581649926644848    F1 = 0.4462272542836215\n",
      "\n",
      "\n",
      "TEST: MAE = 6.694173560018842   Accuracy = 0.4678824538437232    F1 = 0.4503486529077839\n",
      "\n",
      "\n",
      "Train Epoch: 2 [0/88610 (0%)]\tLoss Class: 1.006870 \tLoss Reg: 38.360497\n",
      "Train Epoch: 2 [12800/88610 (14%)]\tLoss Class: 1.011581 \tLoss Reg: 47.797485\n",
      "Train Epoch: 2 [25600/88610 (29%)]\tLoss Class: 1.041454 \tLoss Reg: 57.781414\n",
      "Train Epoch: 2 [38400/88610 (43%)]\tLoss Class: 1.075120 \tLoss Reg: 53.624901\n",
      "Train Epoch: 2 [51200/88610 (58%)]\tLoss Class: 1.028033 \tLoss Reg: 49.566799\n",
      "Train Epoch: 2 [64000/88610 (72%)]\tLoss Class: 1.020111 \tLoss Reg: 64.375977\n",
      "Train Epoch: 2 [76800/88610 (87%)]\tLoss Class: 1.097716 \tLoss Reg: 58.566898\n",
      "TRAIN: MAE = 6.308258836450129   Accuracy = 0.4647556709175037    F1 = 0.4484847694280902\n",
      "\n",
      "\n",
      "TEST: MAE = 6.679978230501114   Accuracy = 0.4783099354489234    F1 = 0.45954005542823456\n",
      "\n",
      "\n",
      "Train Epoch: 3 [0/88610 (0%)]\tLoss Class: 1.016837 \tLoss Reg: 38.100143\n",
      "Train Epoch: 3 [12800/88610 (14%)]\tLoss Class: 1.006840 \tLoss Reg: 47.795971\n",
      "Train Epoch: 3 [25600/88610 (29%)]\tLoss Class: 1.046438 \tLoss Reg: 57.807392\n",
      "Train Epoch: 3 [38400/88610 (43%)]\tLoss Class: 1.072779 \tLoss Reg: 53.567760\n",
      "Train Epoch: 3 [51200/88610 (58%)]\tLoss Class: 1.018709 \tLoss Reg: 49.403358\n",
      "Train Epoch: 3 [64000/88610 (72%)]\tLoss Class: 1.031893 \tLoss Reg: 64.848877\n",
      "Train Epoch: 3 [76800/88610 (87%)]\tLoss Class: 1.106652 \tLoss Reg: 58.394150\n",
      "TRAIN: MAE = 6.3069045325048565   Accuracy = 0.4673964563818982    F1 = 0.45291108336670494\n",
      "\n",
      "\n",
      "TEST: MAE = 6.676577662871963   Accuracy = 0.4768654358326186    F1 = 0.4591973653924061\n",
      "\n",
      "\n",
      "Train Epoch: 4 [0/88610 (0%)]\tLoss Class: 1.031995 \tLoss Reg: 38.046501\n",
      "Train Epoch: 4 [12800/88610 (14%)]\tLoss Class: 1.015051 \tLoss Reg: 47.823624\n",
      "Train Epoch: 4 [25600/88610 (29%)]\tLoss Class: 1.044579 \tLoss Reg: 57.828804\n",
      "Train Epoch: 4 [38400/88610 (43%)]\tLoss Class: 1.053376 \tLoss Reg: 53.484898\n",
      "Train Epoch: 4 [51200/88610 (58%)]\tLoss Class: 1.012669 \tLoss Reg: 49.414749\n",
      "Train Epoch: 4 [64000/88610 (72%)]\tLoss Class: 1.014359 \tLoss Reg: 65.075874\n",
      "Train Epoch: 4 [76800/88610 (87%)]\tLoss Class: 1.104720 \tLoss Reg: 58.375183\n",
      "TRAIN: MAE = 6.305986437470474   Accuracy = 0.46933754655230786    F1 = 0.4564791730740656\n",
      "\n",
      "\n",
      "TEST: MAE = 6.680291362729471   Accuracy = 0.48431363697919017    F1 = 0.46241445371736584\n",
      "\n",
      "\n",
      "Train Epoch: 5 [0/88610 (0%)]\tLoss Class: 1.031076 \tLoss Reg: 38.105251\n",
      "Train Epoch: 5 [12800/88610 (14%)]\tLoss Class: 1.013287 \tLoss Reg: 47.844063\n",
      "Train Epoch: 5 [25600/88610 (29%)]\tLoss Class: 1.060351 \tLoss Reg: 57.821758\n",
      "Train Epoch: 5 [38400/88610 (43%)]\tLoss Class: 1.052927 \tLoss Reg: 53.472767\n",
      "Train Epoch: 5 [51200/88610 (58%)]\tLoss Class: 1.016404 \tLoss Reg: 49.417583\n",
      "Train Epoch: 5 [64000/88610 (72%)]\tLoss Class: 1.019503 \tLoss Reg: 65.101425\n",
      "Train Epoch: 5 [76800/88610 (87%)]\tLoss Class: 1.103279 \tLoss Reg: 58.373222\n",
      "TRAIN: MAE = 6.305791368381211   Accuracy = 0.46948425685588535    F1 = 0.45209465330589393\n",
      "\n",
      "\n",
      "TEST: MAE = 6.681216336121133   Accuracy = 0.4784904979009615    F1 = 0.45737774311772583\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#batch_size = 64\n",
    "epochs = 5\n",
    "log_interval = 200\n",
    "criterion_regression = nn.MSELoss()\n",
    "optimizer_class = optim.Adam(model_classs.parameters(), lr=0.001)\n",
    "optimizer_reg = optim.Adam(model_reg.parameters(), lr=0.001)\n",
    "\n",
    "writer = SummaryWriter(\"/Users/ninelco/Documents/Innopolis/F23/ML/Assignment2\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    trainCascade(model_classs, model_reg, device, train_dataloader, optimizer_class, optimizer_reg, epoch, log_interval)\n",
    "    testCascade(model_classs, model_reg, device, test_dataloader)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6e00d1-7c18-44dc-988d-8d8f8362965f",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b0ecd8-6dae-4b8f-ac87-b9c069951e2a",
   "metadata": {},
   "source": [
    "Was made two new models (multitask and cascade) Was calculated loss, mae (error in minutes), accuracy and f1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64990837-cd0e-44a2-8512-7d79732ed641",
   "metadata": {},
   "source": [
    "batch_size = 64 Adam(lr=0.001) (model 1)\n",
    "\n",
    "*MuktiTask*\n",
    "\n",
    "TEST: MAE = 6.470548434628533   Accuracy = 0.46300726763869454    F1 = 0.4368899685484079 (evaluate stopped - local minimum)\n",
    "\n",
    "*Cascade*\n",
    "\n",
    "TEST: MAE = 6.47104716644507   Accuracy = 0.4819663251026949    F1 = 0.45667140737774714 (evaluate stopped - local minimum)\n",
    "\n",
    "----------\n",
    "batch_size = 32 Adam(lr=0.001) (model 2) - results are similar, calculate more time, better use 64\n",
    "\n",
    "*MuktiTask*\n",
    "\n",
    "TEST: MAE = 6.467062409305985   Accuracy = 0.46887554732993275    F1 = 0.4518291777498136\n",
    "\n",
    "*Cascade*\n",
    "\n",
    "TEST: MAE = 6.487257389795213   Accuracy = 0.4859838396605426    F1 = 0.45839697858374007\n",
    "________\n",
    "batch_size = 64 Adam(lr=0.01) (model 3)\n",
    "\n",
    "*MuktiTask*\n",
    "\n",
    "TEST: MAE = 6.402300580434222   Accuracy = 0.4539791450367896    F1 = 0.4397245795172217\n",
    "\n",
    "*Cascade*\n",
    "\n",
    "TEST: MAE = 6.681216336121133   Accuracy = 0.4784904979009615    F1 = 0.45737774311772583"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c99fd8a-5199-456c-aff5-b78ead68189d",
   "metadata": {},
   "source": [
    "### Another models\n",
    "For upgrade results was changed: optimizers, layers, batch_sizes, epochs, weights of losses - but there are no significant changes. Model find local minimum and can't go out from it. \n",
    "Also try change time_status: if early then to minutes add minus, but it was worse for results. In SGD some times was gradient vanishing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
