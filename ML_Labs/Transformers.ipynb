{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 6 : Transformers and LLMs\n",
        "```\n",
        "- [S23] Advanced Machine Learning, Innopolis University\n",
        "- Teaching Assistant: Gcinizwe Dlamini\n",
        "```\n",
        "<hr>\n",
        "\n",
        "\n",
        "```\n",
        "Lab Plan\n",
        "1. Transformers (translation architecture)\n",
        "2. Self-Attention\n",
        "3. Multi-headed attention\n",
        "4. Positional Encoding\n",
        "5. Transfomer Encoder Part\n",
        "6. Application of Transformers\n",
        "7. Self practice tasks\n",
        "```\n",
        "\n",
        "<hr>\n"
      ],
      "metadata": {
        "id": "_JIu6A55Y0x_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Transformers\n",
        "\n",
        "* [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) -- Original paper on attention\n",
        "\n",
        "![](http://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png)"
      ],
      "metadata": {
        "id": "H4VPwrSfZabH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Transfomer Encoder\n",
        "\n",
        "The encoder contains a self-attention layer – a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. <br>\n",
        "PyTorch implementation : `nn.TransformerEncoder` and `nn.TransformerEncoderLayer` <br>\n",
        "**The main goal is to efficiently encode the data**\n",
        "\n",
        "  1         |  2\n",
        ":-------------------------:|:-------------------------:\n",
        "![](http://jalammar.github.io/images/t/encoder_with_tensors.png)  |  ![](http://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png)\n"
      ],
      "metadata": {
        "id": "oUQqqmffZmgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Self-Attention\n",
        "\n",
        "**Keep in mind : The main goal is to encode the data in a much more efficient way** In other words is to create meaningful embeddings<br>\n",
        "- As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.\n",
        "\n",
        "\n",
        "**How does Self-Attention work?**\n",
        "\n",
        "Steps:\n",
        "1. For each word, we create a **`Query`** vector, a **`Key`** vector, and a **`Value`** vector.\n",
        "  - What are the **`Query`** vector, a **`Key`** vector, and a **`Value`** vector? : They're abstractions that are useful for calculating attention... They are a breakdown of the word embeddings\n",
        "2. Calculating self-attention score from **`Query`** **`Key`** vector.\n",
        "3. Divide the scores by 8 (This leads to having more stable gradients)\n",
        "4. Pass the result through a softmax operation (softmax score determines how much each word will be expressed at this position)\n",
        "5. Multiply each value vector by the softmax score\n",
        "6. Sum up the weighted value vectors"
      ],
      "metadata": {
        "id": "krH5CoxGdHnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1\n",
        "\n",
        "For each word, we create a **`Query`** vector, a **`Key`** vector, and a **`Value`** vector.\n",
        "\n",
        "![](http://jalammar.github.io/images/t/transformer_self_attention_vectors.png)"
      ],
      "metadata": {
        "id": "-H7Wxnp2eTeJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BI7XGeySYtV2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# simple sequence = I will pass AML midterm\n",
        "simple_sequence_embedding = torch.rand((5, 512))\n",
        "\n",
        "# Create weight matrices\n",
        "Wq = torch.normal(0,0.5, (512, 7))\n",
        "Wk = torch.normal(0,0.1, (512, 7))\n",
        "Wv = torch.normal(0,0.2, (512, 7))\n",
        "\n",
        "\n",
        "# Create key, query and value for each word in the senetence\n",
        "queries = simple_sequence_embedding.mm(Wq)\n",
        "keys = simple_sequence_embedding.mm(Wk)\n",
        "values = simple_sequence_embedding.mm(Wv)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "queries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qkWCJpRd3c3",
        "outputId": "b9f435f6-0ce7-45a3-fd1c-d0041060f0c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-5.1061, 11.8408,  2.2086,  2.9840, -2.0887, -0.8397, -1.4995],\n",
              "        [ 2.9208, 11.5278, -4.3229,  2.8188, -4.7808,  2.0832, -1.8955],\n",
              "        [-0.6251, 12.5453,  2.1003, -0.3908, -1.6719,  6.5194,  0.0218],\n",
              "        [-1.6637, 18.6328, -6.2575, -4.3300, -3.0603,  2.0430, -2.3926],\n",
              "        [ 4.7630,  9.6453, -3.1484, -2.0177, -3.1768,  4.7115,  2.6854]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2\n",
        "\n",
        "Calculating self-attention score from **`Query`** and **`Key`** vector"
      ],
      "metadata": {
        "id": "6E3yaeQEed7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = torch.mm(queries, keys.T)\n",
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yABUd2Qd818",
        "outputId": "1475d302-23db-4391-a888-507746e7379a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-36.7789, -38.2997, -27.0507, -59.0658, -52.3270],\n",
              "        [-32.6367, -27.2935, -24.6161, -54.2585, -41.6928],\n",
              "        [-31.3280, -21.5199, -10.8190, -40.1629, -36.4128],\n",
              "        [-54.1099, -47.6417, -44.3191, -80.6077, -79.5127],\n",
              "        [-17.8594, -11.4702,  -4.2672, -26.6263, -21.9530]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3\n",
        "Divide the scores by 8 (This leads to having more stable gradients)"
      ],
      "metadata": {
        "id": "uPM5PoSHer3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = scores/8\n",
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iafga235ekiK",
        "outputId": "7977874f-66fc-4161-d416-31dbdeed77a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ -4.5974,  -4.7875,  -3.3813,  -7.3832,  -6.5409],\n",
              "        [ -4.0796,  -3.4117,  -3.0770,  -6.7823,  -5.2116],\n",
              "        [ -3.9160,  -2.6900,  -1.3524,  -5.0204,  -4.5516],\n",
              "        [ -6.7637,  -5.9552,  -5.5399, -10.0760,  -9.9391],\n",
              "        [ -2.2324,  -1.4338,  -0.5334,  -3.3283,  -2.7441]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4\n",
        "\n",
        "Pass the result through a softmax operation"
      ],
      "metadata": {
        "id": "JsuoztzrexQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = torch.nn.functional.softmax(scores, dim=1)\n",
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0n5NWeN_ez_D",
        "outputId": "cf0f6174-6c35-49b9-da28-e653a49ba413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1850, 0.1530, 0.6241, 0.0114, 0.0265],\n",
              "        [0.1649, 0.3215, 0.4494, 0.0111, 0.0532],\n",
              "        [0.0548, 0.1867, 0.7113, 0.0182, 0.0290],\n",
              "        [0.1487, 0.3339, 0.5058, 0.0054, 0.0062],\n",
              "        [0.1039, 0.2309, 0.5682, 0.0347, 0.0623]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5 & 6\n",
        "\n",
        "* Multiply each value vector by the softmax score\n",
        "* Sum up the weighted value vectors\n",
        "\n"
      ],
      "metadata": {
        "id": "6uzXnkD-e4t4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = scores @ values\n",
        "z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpcDWsBhe6Ma",
        "outputId": "d4287a6a-2610-4a69-d122-ee96d746ac02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 4.4716, -0.8754,  1.0015,  5.3325,  3.6058,  0.7773,  2.0604],\n",
              "        [ 4.5854, -1.1084,  1.0863,  5.2149,  3.6833,  1.1822,  2.2346],\n",
              "        [ 4.4695, -0.9500,  1.0667,  5.4705,  3.5735,  0.9243,  2.1818],\n",
              "        [ 4.6475, -1.0420,  1.0692,  5.2539,  3.7072,  1.2904,  2.2424],\n",
              "        [ 4.4866, -1.0492,  1.1142,  5.3061,  3.5890,  0.9629,  2.2019]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Multi-headed attention\n",
        "\n",
        "**GOAL**:\n",
        "1. Expand the model's ability to focus on different positions\n",
        "2. Provide the attention layer multiple “representation subspaces”\n",
        "\n",
        "**Attention with $N$ just means repeating self attention algorithm $N$ times and joining the results**\n",
        "\n",
        "\n",
        "![](https://data-science-blog.com/wp-content/uploads/2022/01/mha_img_original.png)\n",
        "\n",
        "**Multi-headed attention steps:**\n",
        "1. Same as self-attention calculation, just n different times with different weight matrices\n",
        "2. Condense the $N$ z metrices down into a single matrix by concatinating the matrices then multiply them by an additional weights matrix `WO`\n",
        "\n",
        "Now the output z metrix is fed to the FFNN"
      ],
      "metadata": {
        "id": "7_his4tXgPk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import Tensor\n",
        "import torch.nn.functional as f\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(query, key, value):\n",
        "  temp = query.bmm(key.transpose(1, 2))\n",
        "  scale = query.size(-1) ** 0.5\n",
        "  softmax = f.softmax(temp / scale, dim=-1)\n",
        "  return softmax.bmm(value)"
      ],
      "metadata": {
        "id": "YTwNl_mJfAot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Attention head"
      ],
      "metadata": {
        "id": "pMHINHLkglES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, dim_in, dim_q, dim_k):\n",
        "    super().__init__()\n",
        "    self.q = nn.Linear(dim_in, dim_q)\n",
        "    self.k = nn.Linear(dim_in, dim_k)\n",
        "    self.v = nn.Linear(dim_in, dim_k)\n",
        "\n",
        "  def forward(self, query, key, value):\n",
        "    return scaled_dot_product_attention(self.q(query), self.k(key), self.v(value))"
      ],
      "metadata": {
        "id": "H2QBY47ogoEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Multi Head Attention\n",
        "\n",
        "**Task:** Implement multi-head attention model"
      ],
      "metadata": {
        "id": "gyi73Namg0sM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, number_of_heads, dim_in, dim_q, dim_k):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([AttentionHead(dim_in, dim_q, dim_k) for _ in range(number_of_heads)])\n",
        "    self.linaer = nn.Linear(number_of_heads * dim_k, dim_in)\n",
        "\n",
        "  def forward(self, query, key, value):\n",
        "    z = self.linear(torch.cat([h(query, key, value) for h in self.heads]), dim=-1)\n",
        "    return z"
      ],
      "metadata": {
        "id": "uw01PJMZg3yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Positional Encoding\n",
        "\n",
        "A way to account for the order of the words in the input sequence. A transformer adds a vector to each input embedding which helps it determine the position of each word. <br>\n",
        "**Goal** : preserving information about the order of tokens  \n",
        "\n",
        "Positional Encoding they can either be learned or fixed a priori.\n",
        "\n",
        "Proposed approach from original paper : describe a simple scheme for fixed positional encodings based on sine and cosine functions"
      ],
      "metadata": {
        "id": "6MkY2jNhg2c7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def position_encoding(seq_len, dim_model, device):\n",
        "  pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1, -1, 1)\n",
        "  dim = torch.arange(dim_model, dtype=torch.float, device=device).reshape(1, 1, -1)\n",
        "  phase = pos / (1e4 ** (dim / dim_model))\n",
        "\n",
        "  return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))"
      ],
      "metadata": {
        "id": "cXIr-0-1i0zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](http://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png)"
      ],
      "metadata": {
        "id": "vi3aQYNznBWI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Transfomer Encoder Part\n",
        "### 5.1Encoder Feed Forward"
      ],
      "metadata": {
        "id": "MfCA86ACnH01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feed_forward(dim_input = 512, dim_feedforward = 2048):\n",
        "  return nn.Sequential(nn.Linear(dim_input, dim_feedforward), nn.ReLU(), nn.Linear(dim_feedforward, dim_input))"
      ],
      "metadata": {
        "id": "ee2SB_sInB59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Encoder Residual\n",
        "\n",
        "From the original paper the author implementation"
      ],
      "metadata": {
        "id": "xpQAHDPbnOiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Residual(nn.Module):\n",
        "  def __init__(self, sublayer, dimension, dropout = 0.1):\n",
        "    super().__init__()\n",
        "    self.sublayer = sublayer\n",
        "    self.norm = nn.LayerNorm(dimension)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, *tensors):\n",
        "    return self.norm(tensors[0] + self.dropout(self.sublayer(*tensors)))"
      ],
      "metadata": {
        "id": "j3z2wjR5nRn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Putting the Encoder layer together"
      ],
      "metadata": {
        "id": "zxhYxI4Fnbnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "  def __init__(self, dim_model = 512, num_heads = 6, dim_feedforward = 2048, dropout = 0.1):\n",
        "    super().__init__()\n",
        "    dim_q = dim_k = max(dim_model // num_heads, 1)\n",
        "    self.attention = Residual(MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
        "                              dimension=dim_model, dropout=dropout)\n",
        "    self.feed_forward = Residual(feed_forward(dim_model, dim_feedforward), dimension=dim_model, dropout=dropout)\n",
        "\n",
        "  def forward(self, src):\n",
        "    src = self.attention(src, src, src)\n",
        "    return self.feed_forward(src)"
      ],
      "metadata": {
        "id": "lZz2JhflniHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4 Putting together transfomer Encoder part"
      ],
      "metadata": {
        "id": "oUGS-3opnlxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, num_layers = 12, dim_model = 512, num_heads = 4, dim_feedforward = 2048, dropout: float = 0.1):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList([TransformerEncoderLayer(dim_model, num_heads, dim_feedforward, dropout) for _ in range(num_layers) ])\n",
        "\n",
        "  def forward(self, src):\n",
        "    seq_len, dimension = src.size(1), src.size(2)\n",
        "    src += position_encoding(seq_len, dimension)\n",
        "    for layer in self.layers:\n",
        "      src = layer(src)\n",
        "\n",
        "    return src"
      ],
      "metadata": {
        "id": "QplT81gNnie2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Decoder Side\n",
        "\n",
        "The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder.\n",
        "\n",
        "The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.\n",
        "\n",
        "![](https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png)\n"
      ],
      "metadata": {
        "id": "GuafKkNanzV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder layer\n",
        "\n",
        "\n",
        "\n",
        "**Task**: implement the decoder layer"
      ],
      "metadata": {
        "id": "fjwjs_zzn5Le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "  def __init__(self, dim_model = 512, num_heads = 4, dim_feedforward = 2048, dropout = 0.1):\n",
        "    super().__init__()\n",
        "    dim_q = dim_k = max(dim_model // num_heads, 1)\n",
        "    self.self_attention = Residual(MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
        "                                   dimension=dim_model, dropout=dropout)\n",
        "    self.encoder_attention = Residual(MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
        "                                      dimension=dim_model, dropout=dropout)\n",
        "    self.feed_forward = Residual(feed_forward(dim_model, dim_feedforward), dimension=dim_model, dropout=dropout)\n",
        "\n",
        "  def forward(self, tgt, memory, src_mask=None, tgt_mask=None):\n",
        "    tgt = self.self_attention(tgt, tgt, tgt, mask=tgt_mask)\n",
        "    tgt = self.encoder_attention(tgt, memory, memory, mask=src_mask)\n",
        "    return self.feed_forward(tgt)"
      ],
      "metadata": {
        "id": "ZfrwtIdOn1pT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, num_layers=6, dim_model=512, num_heads=6, dim_feedforward=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([TransformerDecoderLayer(dim_model, num_heads, dim_feedforward, dropout) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, tgt, memory, src_mask=None, tgt_mask=None):\n",
        "        seq_len, dimension = tgt.size(1), tgt.size(2)\n",
        "        tgt += position_encoding(seq_len, dimension)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            tgt = layer(tgt, memory, src_mask, tgt_mask)\n",
        "        return tgt"
      ],
      "metadata": {
        "id": "sMZTBDDgQlPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_encoder_layers=6, num_decoder_layers=6, dim_model=512, num_heads=6, dim_feedforward=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = TransformerEncoder(num_encoder_layers, dim_model, num_heads, dim_feedforward, dropout)\n",
        "        self.decoder = TransformerDecoder(num_decoder_layers, dim_model, num_heads, dim_feedforward, dropout)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        memory = self.encoder(src)\n",
        "        output = self.decoder(tgt, memory, src_mask, tgt_mask)\n",
        "        return output"
      ],
      "metadata": {
        "id": "bF6C_4gmXYNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Application of Transfomers\n",
        "\n",
        "We will look at sentiment analysis"
      ],
      "metadata": {
        "id": "5nafOjPVpsDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self, output_dim):\n",
        "    super().__init__()\n",
        "    self.transformer = transformers.AutoModel.from_pretrained('bert-base-uncased')\n",
        "    for param in self.transformer.parameters():\n",
        "        param.requires_grad = False #only extract information\n",
        "\n",
        "    hidden_dim = self.transformer.config.hidden_size\n",
        "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "\n",
        "  def forward(self, text):\n",
        "    # text = [batch size, seq len]\n",
        "    output = self.transformer(text, output_attentions=True)\n",
        "    hidden = output.last_hidden_state\n",
        "    # hidden = [batch size, seq len, hidden dim]\n",
        "    attention = output.attentions[-1]\n",
        "    # attention = [batch size, n heads, seq len, seq len]\n",
        "    cls_hidden = hidden[:, 0, :]\n",
        "    prediction = self.fc(torch.tanh(cls_hidden))\n",
        "\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "_mcCQsUQp4-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Tasks\n",
        "\n",
        "```\n",
        "Task 1\n",
        "- Using the above implementation code the decoder layer and assemble a full transformer model\n",
        "```\n",
        "\n",
        "<hr>\n",
        "\n",
        "```\n",
        "Task 2\n",
        "- Implement, train and test a Transfomer model for Part-of-speech tagging task.\n",
        "```\n",
        "\n",
        "**Task 2 Datasets**: [Train](https://www.dropbox.com/s/x9n6f9o9jl7pno8/train_pos.txt?dl=1), [Test](https://www.dropbox.com/s/v8nccvq7jewcl8s/test_pos.txt?dl=1)\n"
      ],
      "metadata": {
        "id": "GqLHQt0iog5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class POSDataset(Dataset):\n",
        "    def __init__(self, file_path, label_encoder, train_encoder = False):\n",
        "        self.sentences = []\n",
        "        self.tags = []\n",
        "        self.sentence = []\n",
        "        self.tag = []\n",
        "\n",
        "        with open(file_path, 'r') as file:\n",
        "            for line in file:\n",
        "                if line.isspace() and self.sentence:\n",
        "                  self.sentences.append(self.sentence)\n",
        "                  self.sentence = []\n",
        "                  self.tags.append(self.tag)\n",
        "                  self.tag = []\n",
        "                  continue\n",
        "\n",
        "                word, label = line.lower().strip().split()\n",
        "                self.sentence.append(word)\n",
        "                self.tag.append(label)\n",
        "\n",
        "        if train_encoder:\n",
        "          labels = [item for sublist in self.tags for item in sublist]\n",
        "          label_encoder.fit(labels)\n",
        "          print(self.sentences[0])\n",
        "          print(self.tags[0])\n",
        "          print(self.sentences[-1])\n",
        "          print(label_encoder.transform(self.tags[-1]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      return self.sentences[idx], self.tags[idx]\n"
      ],
      "metadata": {
        "id": "H9m7y3Eur0Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "label_encoder = LabelEncoder()\n",
        "file_path_train = 'train_pos.txt'\n",
        "train_dataset = POSDataset(file_path_train, label_encoder, train_encoder = True)\n",
        "\n",
        "file_path_test = 'test_pos.txt'\n",
        "test_dataset = POSDataset(file_path_test, label_encoder)\n",
        "\n",
        "min_freq = 5\n",
        "special_tokens = [\"<unk>\", \"<pad>\"]\n",
        "\n",
        "def get_tokens(dataset):\n",
        "    for idx in range(len(dataset)):\n",
        "        yield dataset[idx][0]\n",
        "\n",
        "vocab = torchtext.vocab.build_vocab_from_iterator(\n",
        "    get_tokens(train_dataset),\n",
        "    min_freq=min_freq,\n",
        "    specials=special_tokens,\n",
        ")\n",
        "\n",
        "unk_index = vocab[\"<unk>\"]\n",
        "pad_index = vocab[\"<pad>\"]\n",
        "vocab.set_default_index(unk_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7p0cl4WgE_T0",
        "outputId": "5d0738be-9194-4a76-bc99-eaccccb0359b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['confidence', 'in', 'the', 'pound', 'is', 'widely', 'expected', 'to', 'take', 'another', 'sharp', 'dive', 'if', 'trade', 'figures', 'for', 'september', ',', 'due', 'for', 'release', 'tomorrow', ',', 'fail', 'to', 'show', 'a', 'substantial', 'improvement', 'from', 'july', 'and', 'august', \"'s\", 'near-record', 'deficits', '.']\n",
            "['nn', 'in', 'dt', 'nn', 'vbz', 'rb', 'vbn', 'to', 'vb', 'dt', 'jj', 'nn', 'in', 'nn', 'nns', 'in', 'nnp', ',', 'jj', 'in', 'nn', 'nn', ',', 'vb', 'to', 'vb', 'dt', 'jj', 'nn', 'in', 'nnp', 'cc', 'nnp', 'pos', 'jj', 'nns', '.']\n",
            "['it', 'is', 'also', 'pulling', '20', 'people', 'out', 'of', 'puerto', 'rico', ',', 'who', 'were', 'helping', 'huricane', 'hugo', 'victims', ',', 'and', 'sending', 'them', 'to', 'san', 'francisco', 'instead', '.']\n",
            "[25 39 27 36 10 22 14 14 20 20  5 41 35 36 20 20 22  5  9 36 25 32 20 20\n",
            " 27  6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "special_tokens = [\"<pad>\"]\n",
        "\n",
        "def get_tags(dataset):\n",
        "    for idx in range(len(dataset)):\n",
        "        yield dataset[idx][1]\n",
        "\n",
        "tag_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
        "    get_tags(train_dataset),\n",
        "    min_freq=min_freq,\n",
        "    specials=special_tokens,\n",
        ")\n",
        "\n",
        "tag_pad_index = tag_vocab[\"<pad>\"]"
      ],
      "metadata": {
        "id": "lrUKNnWvFLnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sample(example, vocab, tag_vocab):\n",
        "    ids = vocab.lookup_indices(example[0])\n",
        "    tag_ids = tag_vocab.lookup_indices(example[1])\n",
        "    return ids, tag_ids"
      ],
      "metadata": {
        "id": "E4gPP4IuFQQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class POSDatasetEncoded(Dataset):\n",
        "    def __init__(self, pos_dataset, vocab, tag_vocab, padded_len=78):\n",
        "\n",
        "        self.sentences = []\n",
        "        self.tags = []\n",
        "        for idx in range(len(pos_dataset)):\n",
        "            sample = pos_dataset[idx]\n",
        "            ids, tag_ids = encode_sample(sample, vocab, tag_vocab)\n",
        "            l = len(ids)\n",
        "\n",
        "            ids += [vocab['<pad>']] * (padded_len - l)\n",
        "            tag_ids += [tag_vocab['<pad>']] * (padded_len - l)\n",
        "\n",
        "            self.sentences.append(torch.LongTensor(ids))\n",
        "            self.tags.append(torch.LongTensor(tag_ids))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sentences[idx], self.tags[idx]\n",
        "\n",
        "train_dataset = POSDatasetEncoded(train_dataset, vocab, tag_vocab)\n",
        "test_dataset = POSDatasetEncoded(test_dataset, vocab, tag_vocab)"
      ],
      "metadata": {
        "id": "XCGRRqhpFSIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, persistent_workers=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, persistent_workers=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsjmBubjFUPx",
        "outputId": "fc13c50f-c7b3-4943-ba0f-399d0960935c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "A5TF2cL4HjpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "from tempfile import TemporaryDirectory\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
        "                 nlayers: int, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.embedding = nn.Embedding(ntoken, d_model)\n",
        "        self.d_model = d_model\n",
        "        self.linear = nn.Linear(d_model, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.linear.bias.data.zero_()\n",
        "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            src: Tensor, shape ``[seq_len, batch_size]``\n",
        "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
        "\n",
        "        Returns:\n",
        "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
        "        \"\"\"\n",
        "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        if src_mask is None:\n",
        "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
        "            Unmasked positions are filled with float(0.0).\n",
        "            \"\"\"\n",
        "            src_mask = nn.Transformer.generate_square_subsequent_mask(len(src)).to(device)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.linear(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "jahq9zxmHbYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "ntokens = len(vocab)  # size of vocabulary\n",
        "emsize = 30  # embedding dimension\n",
        "d_hid = 200  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
        "nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
        "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
        "dropout = 0.2  # dropout probability\n",
        "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
      ],
      "metadata": {
        "id": "tIrCSQh4Gwbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import tqdm\n",
        "import numpy as np\n",
        "lr = 5e-4\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "-TI3_qW-HJ4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 10\n",
        "for ep in range(n_epochs):\n",
        "  model.train()\n",
        "  epoch_losses = []\n",
        "  train_correct = 0\n",
        "  train_total = 0\n",
        "  for batch in tqdm.tqdm(train_loader, desc=\"training...\", leave=False):\n",
        "    optimizer.zero_grad()\n",
        "    ids = batch[0].to(device)\n",
        "    label = batch[1].to(device)\n",
        "\n",
        "    prediction = model(ids)\n",
        "    #output_flat = output.view(-1, ntokens)\n",
        "    loss = criterion(prediction.view(-1, prediction.shape[2]), label.view(-1))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_losses.append(loss.item())\n",
        "\n",
        "    _, predicted_tags = torch.max(prediction, 2)\n",
        "    train_total += prediction.shape[0] * prediction.shape[1]\n",
        "    train_correct += (predicted_tags == label).sum().item()\n",
        "\n",
        "  test_losses = []\n",
        "  test_correct = 0\n",
        "  test_total = 0\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    for batch in tqdm.tqdm(test_loader, desc='evaluating...', leave=False):\n",
        "      ids = batch[0].to(device)\n",
        "      label = batch[1].to(device)\n",
        "\n",
        "      prediction = model(ids)\n",
        "      loss = criterion(prediction.view(-1, prediction.shape[2]), label.view(-1))\n",
        "      test_losses.append(loss.item())\n",
        "\n",
        "      _, predicted_tags = torch.max(prediction, 2)\n",
        "      test_total += prediction.shape[0] * prediction.shape[1]\n",
        "      test_correct += (predicted_tags == label).sum().item()\n",
        "\n",
        "\n",
        "  print(f'[Epoch {ep}] Train:\\n\\tLoss: {np.mean(epoch_losses)}, Acc: {train_correct / train_total}\\nTest:\\n\\tLoss: {np.mean(test_losses)}, Acc: {test_correct / test_total}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Nom-1YQImhX",
        "outputId": "772b8601-f318-4c1b-8607-6a774d2ad8cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0] Train:\n",
            "\tLoss: 0.1148509298584291, Acc: 0.9591625921079815\n",
            "Test:\n",
            "\tLoss: 0.1327374642567029, Acc: 0.9536626395473314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Train:\n",
            "\tLoss: 0.11479813706661973, Acc: 0.9591941555907536\n",
            "Test:\n",
            "\tLoss: 0.1327663745198931, Acc: 0.9536690115715961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 2] Train:\n",
            "\tLoss: 0.11481394818318742, Acc: 0.9592041985170902\n",
            "Test:\n",
            "\tLoss: 0.1327995296035494, Acc: 0.9537263597899781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 3] Train:\n",
            "\tLoss: 0.11474067643284798, Acc: 0.9591224204026353\n",
            "Test:\n",
            "\tLoss: 0.1328321217544495, Acc: 0.9537837080083601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 4] Train:\n",
            "\tLoss: 0.114547873900405, Acc: 0.959297454261644\n",
            "Test:\n",
            "\tLoss: 0.13286202270833272, Acc: 0.9537136157414488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 5] Train:\n",
            "\tLoss: 0.1143162717510547, Acc: 0.9592314578885751\n",
            "Test:\n",
            "\tLoss: 0.13288922915382992, Acc: 0.9537454758627721\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 6] Train:\n",
            "\tLoss: 0.11459328957966396, Acc: 0.9591324633289718\n",
            "Test:\n",
            "\tLoss: 0.13291437521813407, Acc: 0.9536753835958607\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 7] Train:\n",
            "\tLoss: 0.11445691234299114, Acc: 0.9593046277804559\n",
            "Test:\n",
            "\tLoss: 0.13293731023394872, Acc: 0.9536371514502727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 8] Train:\n",
            "\tLoss: 0.1143775873152273, Acc: 0.9591195509951105\n",
            "Test:\n",
            "\tLoss: 0.13295724550409924, Acc: 0.9536180353774787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                               "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 9] Train:\n",
            "\tLoss: 0.11472529510834388, Acc: 0.9591181162913481\n",
            "Test:\n",
            "\tLoss: 0.13296770730188914, Acc: 0.9536052913289493\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    }
  ]
}